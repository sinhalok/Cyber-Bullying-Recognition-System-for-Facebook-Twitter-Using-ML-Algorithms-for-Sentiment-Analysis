{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-420a2e4885b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# from nltk.tokenize import word_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOAuthHandler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Basic Packages\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Text Preprocessing Packages\n",
    "import re\n",
    "import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import tweepy \n",
    "from tweepy.auth import OAuthHandler\n",
    "from textblob import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name):\n",
    "    #name='gandhi'\n",
    "    def initialize(): \n",
    "\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'snseusIoIioTvEpDaBcPjUryw'\n",
    "        consumer_secret = 'cDhGVySW9xRUQSbc2o8yKMHfAxBrnIBvE1wSaWoz1PIBXspFTm'\n",
    "        access_token = '2856915038-5xVKLBpmMhX3l4uHBZfmdmIRtXGt1Q0K8yUrexR'\n",
    "        access_token_secret = 'uiHGPQZuxr9z0bsnaPMPgdMemk8oXOwUYjSLJfsT2VmCM'\n",
    "\n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            api = tweepy.API(auth) \n",
    "            print('Authentication Success')\n",
    "            return(api)\n",
    "\n",
    "        except Exception as e: \n",
    "            print(\"Error: Authentication Failed\")\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    def get_tweets(api, query, count = 100): \n",
    "            # empty list to store parsed tweets \n",
    "            tweets = [] \n",
    "            sinceId = None\n",
    "            max_id = -1\n",
    "            tweetCount = 0\n",
    "            tweetsPerQry = 100\n",
    "\n",
    "            while tweetCount < count:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = api.search(q=query, count=tweetsPerQry,lang='en')\n",
    "                        else:\n",
    "                            new_tweets = api.search(q=query, count=tweetsPerQry,\n",
    "                                                    since_id=sinceId,lang='en')\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = api.search(q=query, count=tweetsPerQry,\n",
    "                                                    max_id=str(max_id - 1),lang='en')\n",
    "                        else:\n",
    "                            new_tweets = api.search(q=query, count=tweetsPerQry,\n",
    "                                                    max_id=str(max_id - 1),\n",
    "                                                    since_id=sinceId,lang='en')\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break\n",
    "\n",
    "                    for tweet in new_tweets:\n",
    "                        parsed_tweet = {} \n",
    "                        parsed_tweet['tweets'] = tweet.text\n",
    "                        parsed_tweet['date'] = tweet.created_at\n",
    "\n",
    "                        # saving sentiment of tweet \n",
    "                        parsed_tweet['cleaned_tweets'],parsed_tweet['sentiment_score'],parsed_tweet['sentiment'] = get_sentiment(tweet.text)\n",
    "                        #parsed_tweet['sentiments'] = [tag_sentiment(tweet.text)]\n",
    "                        # appending parsed tweet to tweets list \n",
    "                        if tweet.retweet_count > 0: \n",
    "                            # if tweet has retweets, ensure that it is appended only once \n",
    "                            if parsed_tweet not in tweets: \n",
    "                                tweets.append(parsed_tweet) \n",
    "                        else: \n",
    "                            tweets.append(parsed_tweet) \n",
    "\n",
    "                    tweetCount += len(new_tweets)\n",
    "                    #print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                   # print(max_id)\n",
    "                   # print(new_tweets[-1])\n",
    "                    return tweets\n",
    "                except tweepy.TweepError as e:\n",
    "                    print(\"Tweepy error : \" + str(e))\n",
    "\n",
    "    api_initialization = initialize()\n",
    "    \n",
    "    def clean_tweet(tweets): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        #print(tweets)\n",
    "        return(' '.join(re.sub(\"([,\\.():;!$%^&*\\d])|([^0-9A-Za-z \\t])\", \" \", tweets).split())) \n",
    "\n",
    "    def penn_to_wn(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        elif tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        return None\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_sentiment(text):\n",
    "        \"\"\" returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \"\"\"\n",
    "        cleaned_tweets = preprocess_tweet(text)\n",
    "        tagged = nltk.pos_tag(word_tokenize(cleaned_tweets))\n",
    "        sentiment_score = 0.0\n",
    "        tokens_count = 0\n",
    "        sentiment = []\n",
    "        for word, tag in tagged:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            synsets = wn.synsets(word, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "\n",
    "        # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "    #         print(synset)\n",
    "    #         print(synset.name())\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "    #         print(swn_synset)\n",
    "            sentiment_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        \n",
    "            tokens_count += 1\n",
    "        if not tokens_count:\n",
    "            sentiment.append('neutral')\n",
    " \n",
    "    # sum greater than 0 => positive sentiment\n",
    "        if sentiment_score >= 0:\n",
    "            sentiment.append('positive')\n",
    "        else:\n",
    "            sentiment.append('negative')\n",
    "        # negative sentiment\n",
    "\n",
    "\n",
    "        return cleaned_tweets,sentiment_score, sentiment.pop()\n",
    "\n",
    "    contractions = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "    def expand_contractions(text):\n",
    "        for word in text.split():\n",
    "            if word.lower() in contractions:\n",
    "                text = text.replace(word, contractions[word.lower()])\n",
    "        return text\n",
    "\n",
    "    def preprocess_word(word):\n",
    "        # Remove punctuation\n",
    "        word = word.strip('\"?!,.():;')\n",
    "        # Convert more than 2 letter repetitions to 2 letter\n",
    "        # funnnnny --> funny\n",
    "        word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "        # Remove - & '\n",
    "        word = re.sub(r'(-)', '', word)\n",
    "        return word\n",
    "\n",
    "    def is_valid_word(word):\n",
    "        # Check if word begins with an alphabet\n",
    "        return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "    def handle_emojis(tweet):\n",
    "        # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "        tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "        # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "        tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "        # Love -- <3, :*\n",
    "        tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "        # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "        tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "        tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "        # Cry -- :,(, :'(, :\"(\n",
    "        tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "        return tweet\n",
    "    from nltk import WordNetLemmatizer\n",
    "\n",
    "    def preprocess_tweet(tweet):\n",
    "        processed_tweet = []\n",
    "        # Convert to lower case\n",
    "        tweet = tweet.lower()\n",
    "        tweet = expand_contractions(re.sub('’', \"'\", tweet))\n",
    "        # Replaces URLs with the word URL\n",
    "        tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "        # Replace @handle with the word USER_MENTION\n",
    "        tweet = re.sub(r'@[\\S]+', r' ', tweet)\n",
    "        # Replaces #hashtag with hashtag\n",
    "        tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "        # Remove RT (retweet)\n",
    "        tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "        # Replace 2+ dots with space\n",
    "        tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "        # Strip space, \" and ' from tweet\n",
    "        #tweet = tweet.strip(' \"\\'')\n",
    "        # Replace emojis with either EMO_POS or EMO_NEG\n",
    "        tweet = handle_emojis(tweet)\n",
    "        # Replace multiple spaces with a single space\n",
    "        tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "        tweet = re.sub(r'(\\[|\\])',' ', tweet)\n",
    "        \n",
    "        words = tweet.split()\n",
    "\n",
    "        for word in words:\n",
    "            word = preprocess_word(word)\n",
    "            if is_valid_word(word):\n",
    "                #if use_stemmer:\n",
    "                word = str(WordNetLemmatizer().lemmatize(word))\n",
    "                processed_tweet.append(word)\n",
    "\n",
    "        return ' '.join(processed_tweet)\n",
    "\n",
    "    global retreived_tweets\n",
    "    retreived_tweets = get_tweets(api=api_initialization,query=name, count = 100)\n",
    "\n",
    "        \n",
    "    return retreived_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication Success\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    get_data('OnePlus 8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @OnePlus_IN: Planning to buy the #OnePlus9P...</td>\n",
       "      <td>2021-03-31 08:36:14</td>\n",
       "      <td>planning to buy the oneplus9pro5g we have some...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @OnePlus_IN: Planning to buy the #OnePlus9P...</td>\n",
       "      <td>2021-03-31 08:26:27</td>\n",
       "      <td>planning to buy the oneplus9pro5g we have some...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OnePlus 8 Glacial Green, 5G Unlocked Android S...</td>\n",
       "      <td>2021-03-31 08:25:26</td>\n",
       "      <td>oneplus glacial green unlocked android smartph...</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @OnePlus_IN: Planning to buy the #OnePlus9P...</td>\n",
       "      <td>2021-03-31 08:24:05</td>\n",
       "      <td>planning to buy the oneplus9pro5g we have some...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @OnePlus_IN: Gear up for the Red Cable Firs...</td>\n",
       "      <td>2021-03-31 08:21:54</td>\n",
       "      <td>gear up for the red cable first sale sale begi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RT @OnePlus_IN: Gear up for the Red Cable Firs...</td>\n",
       "      <td>2021-03-31 04:12:04</td>\n",
       "      <td>gear up for the red cable first sale sale begi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@geekyranjit I think last good oneplus mobile ...</td>\n",
       "      <td>2021-03-31 04:08:14</td>\n",
       "      <td>i think last good oneplus mobile wa beyond tha...</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>#mobilezon #mobilezonshop #mobileaccessories #...</td>\n",
       "      <td>2021-03-31 04:03:18</td>\n",
       "      <td>mobilezon mobilezonshop mobileaccessories appl...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>@geekyranjit If I'm getting a OnePlus 8 Pro ar...</td>\n",
       "      <td>2021-03-31 03:58:34</td>\n",
       "      <td>if i am getting a oneplus pro around used is i...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RT @OnePlus_IN: Gear up for the Red Cable Firs...</td>\n",
       "      <td>2021-03-31 03:56:27</td>\n",
       "      <td>gear up for the red cable first sale sale begi...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweets                date  \\\n",
       "0   RT @OnePlus_IN: Planning to buy the #OnePlus9P... 2021-03-31 08:36:14   \n",
       "1   RT @OnePlus_IN: Planning to buy the #OnePlus9P... 2021-03-31 08:26:27   \n",
       "2   OnePlus 8 Glacial Green, 5G Unlocked Android S... 2021-03-31 08:25:26   \n",
       "3   RT @OnePlus_IN: Planning to buy the #OnePlus9P... 2021-03-31 08:24:05   \n",
       "4   RT @OnePlus_IN: Gear up for the Red Cable Firs... 2021-03-31 08:21:54   \n",
       "..                                                ...                 ...   \n",
       "95  RT @OnePlus_IN: Gear up for the Red Cable Firs... 2021-03-31 04:12:04   \n",
       "96  @geekyranjit I think last good oneplus mobile ... 2021-03-31 04:08:14   \n",
       "97  #mobilezon #mobilezonshop #mobileaccessories #... 2021-03-31 04:03:18   \n",
       "98  @geekyranjit If I'm getting a OnePlus 8 Pro ar... 2021-03-31 03:58:34   \n",
       "99  RT @OnePlus_IN: Gear up for the Red Cable Firs... 2021-03-31 03:56:27   \n",
       "\n",
       "                                       cleaned_tweets  sentiment_score  \\\n",
       "0   planning to buy the oneplus9pro5g we have some...            0.375   \n",
       "1   planning to buy the oneplus9pro5g we have some...            0.375   \n",
       "2   oneplus glacial green unlocked android smartph...           -0.500   \n",
       "3   planning to buy the oneplus9pro5g we have some...            0.375   \n",
       "4   gear up for the red cable first sale sale begi...            0.000   \n",
       "..                                                ...              ...   \n",
       "95  gear up for the red cable first sale sale begi...            0.000   \n",
       "96  i think last good oneplus mobile wa beyond tha...           -0.250   \n",
       "97  mobilezon mobilezonshop mobileaccessories appl...            0.000   \n",
       "98  if i am getting a oneplus pro around used is i...            0.000   \n",
       "99  gear up for the red cable first sale sale begi...            0.000   \n",
       "\n",
       "   sentiment  \n",
       "0   positive  \n",
       "1   positive  \n",
       "2   negative  \n",
       "3   positive  \n",
       "4   positive  \n",
       "..       ...  \n",
       "95  positive  \n",
       "96  negative  \n",
       "97  positive  \n",
       "98  positive  \n",
       "99  positive  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(retreived_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
